{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cliff Walking\n",
    "### Sarsa Vs. Q-learning\n",
    "#### The Cliff env:\n",
    "<img src=\"cliff.png\" alt=\"the cliff\" width=\"500\" />\n",
    "\n",
    "#### Choice of actions:\n",
    " - 0: up\n",
    " - 1: down\n",
    " - 2: right\n",
    " - 3: left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the cliff environment\n",
    "rows = 4\n",
    "columns = 12\n",
    "START = (3, 0)\n",
    "GOAL = (3, 11)\n",
    "num_actions = 4\n",
    "\n",
    "# 0- up, 1- down, 2- right, 3- left\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_step(state, action):\n",
    "    next_state = state\n",
    "    if action == 0 and state[0]>0:\n",
    "            next_state = (state[0]-1, state[1])\n",
    "    elif action == 1 and state[0]<rows-1:\n",
    "            next_state = (state[0]+1, state[1])\n",
    "    elif action == 2 and state[1]<columns-1:\n",
    "            next_state = (state[0], state[1]+1)\n",
    "    elif action == 3 and state[1]>0:\n",
    "            next_state = (state[0], state[1]-1)\n",
    "    # reward is -1 for each step unless you fall in the cliff in which case you get a reward of -100\n",
    "    if next_state[0] == rows-1 and next_state[1] != 0 and next_state[1]!= columns-1:\n",
    "        reward = -100\n",
    "    else:\n",
    "        reward = -1\n",
    "    # check if the episode is terminated i.e: reached goal or fell into the cliff\n",
    "    terminated = False\n",
    "    if next_state[0] == rows-1 and next_state[1] != 0:\n",
    "        terminated = True\n",
    "    \n",
    "    return [next_state, reward, terminated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(alpha, epsilon, gamma, episodes) :\n",
    "    Q = np.zeros((rows, columns, num_actions))\n",
    "    for episode in range(episodes):\n",
    "        curr_state = START\n",
    "        # choosing action based on epsion greedy policy over Q\n",
    "        if np.random.random()>epsilon:\n",
    "            # exploit greedy action\n",
    "            curr_action = np.argmax(Q[curr_state[0]][curr_state[1]])\n",
    "        else:\n",
    "            #explore random action\n",
    "            curr_action = np.random.randint(0,num_actions)\n",
    "        terminated = False\n",
    "        while  not terminated:\n",
    "            next_state, reward, terminated = env_step(curr_state, curr_action)\n",
    "            # choosing next action based on epsilon greedy poicy over Q\n",
    "            if np.random.random()>epsilon:\n",
    "                next_action = np.argmax(Q[next_state])\n",
    "            else:\n",
    "                next_action = np.random.randint(0, num_actions)\n",
    "            # updating Q\n",
    "            Q[curr_state[0]][curr_state[1]][curr_action] += alpha*(reward + gamma*Q[next_state[0]][next_state[1]][next_action] - Q[curr_state[0]][curr_state[1]][curr_action])\n",
    "            curr_state = next_state\n",
    "            curr_action = next_action\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning(alpha, epsilon, gamma, episodes):\n",
    "    Q = np.zeros((rows, columns, num_actions))\n",
    "    for episode in range(episodes): \n",
    "        curr_state = START\n",
    "        terminated = False\n",
    "        while  not terminated:\n",
    "            if np.random.random()>epsilon:\n",
    "                curr_action = np.argmax(Q[curr_state[0]][curr_state[1]])\n",
    "            else:\n",
    "                curr_action = np.random.randint(0, num_actions)\n",
    "            next_state, reward, terminated = env_step(curr_state, curr_action)\n",
    "            Q[curr_state[0]][curr_state[1]][curr_action] += alpha*(reward + gamma*np.max(Q[next_state[0]][next_state[1]]) - Q[curr_state[0]][curr_state[1]][curr_action])\n",
    "            curr_state = next_state\n",
    "    return Q\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter variables\n",
    "alpha = 0.1\n",
    "epsilon = 0.1\n",
    "gamma = 1 # undiscounted task\n",
    "episodes = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(Q):\n",
    "    pi = np.zeros((rows, columns))\n",
    "    for i in range(rows):\n",
    "        for j in range(columns):\n",
    "            pi[i][j] = np.argmax(Q[i][j])\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(policy):\n",
    "    curr_state = START\n",
    "    R = 0\n",
    "    terminated = False\n",
    "    while not terminated:\n",
    "        curr_state, reward, terminated= env_step(curr_state, policy[curr_state[0]][curr_state[1]])\n",
    "        R+=reward\n",
    "    return R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_func_sarsa = sarsa(alpha, epsilon, gamma, episodes)\n",
    "# print (\"value function after sarsa:  \\n\\n\", value_func )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy learnt from sarsa: \n",
      "\n",
      " [[2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1.]\n",
      " [0. 0. 0. 0. 0. 2. 2. 2. 2. 2. 2. 1.]\n",
      " [0. 0. 0. 2. 0. 0. 0. 2. 0. 0. 2. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "policy_sarsa = greedy_policy(value_func_sarsa)\n",
    "print (\"policy learnt from sarsa: \\n\\n\", policy_sarsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "return from following optimal policy from sarsa:  -17\n"
     ]
    }
   ],
   "source": [
    "print (\"return from following optimal policy from sarsa: \", run(policy_sarsa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_func_Q = Q_learning(alpha, epsilon, gamma, episodes)\n",
    "# print (\"value function after sarsa:  \\n\\n\", value_func )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy learnt from Q-learning: \n",
      "\n",
      " [[2. 2. 1. 2. 2. 2. 1. 1. 1. 1. 2. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "policy_Q = greedy_policy(value_func_Q)\n",
    "print (\"policy learnt from Q-learning: \\n\\n\", policy_Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "return from following optimal policy from Q-learning:  -13\n"
     ]
    }
   ],
   "source": [
    "print (\"return from following optimal policy from Q-learning: \", run(policy_Q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sarsa learns the safer path while Q-learning learns the optimal path\n",
    "<img src=\"cliff_path.png\" alt=\"cliff paths\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
